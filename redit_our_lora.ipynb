{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:45.974765Z","iopub.status.busy":"2024-05-14T15:13:45.974207Z","iopub.status.idle":"2024-05-14T15:13:53.067866Z","shell.execute_reply":"2024-05-14T15:13:53.066880Z","shell.execute_reply.started":"2024-05-14T15:13:45.974726Z"},"trusted":true},"outputs":[],"source":["import os\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from datasets import load_dataset\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:53.070162Z","iopub.status.busy":"2024-05-14T15:13:53.069597Z","iopub.status.idle":"2024-05-14T15:13:53.094383Z","shell.execute_reply":"2024-05-14T15:13:53.093534Z","shell.execute_reply.started":"2024-05-14T15:13:53.070126Z"},"trusted":true},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:53.096009Z","iopub.status.busy":"2024-05-14T15:13:53.095680Z","iopub.status.idle":"2024-05-14T15:13:53.114508Z","shell.execute_reply":"2024-05-14T15:13:53.113789Z","shell.execute_reply.started":"2024-05-14T15:13:53.095978Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","NUM_BATCHES = 500\n","EPOCHS = 3\n","MAX_SEQUENCE_LENGTH = 128\n","MAX_GENERATION_LENGTH = 100\n","GPT2_PRESET = \"gpt2\"\n","RANK = 8\n","ALPHA = 32.0"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:53.117524Z","iopub.status.busy":"2024-05-14T15:13:53.117131Z","iopub.status.idle":"2024-05-14T15:13:59.127508Z","shell.execute_reply":"2024-05-14T15:13:59.126698Z","shell.execute_reply.started":"2024-05-14T15:13:53.117492Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 57.8M/57.8M [00:02<00:00, 21.7MB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b19a7d9c349c488dac4489073350d51a","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/42139 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["reddit_ds = load_dataset(\"reddit_tifu\", \"long\", split=\"train\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:59.128863Z","iopub.status.busy":"2024-05-14T15:13:59.128579Z","iopub.status.idle":"2024-05-14T15:13:59.135285Z","shell.execute_reply":"2024-05-14T15:13:59.134259Z","shell.execute_reply.started":"2024-05-14T15:13:59.128839Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'],\n","    num_rows: 42139\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["reddit_ds"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:59.136785Z","iopub.status.busy":"2024-05-14T15:13:59.136422Z","iopub.status.idle":"2024-05-14T15:13:59.299142Z","shell.execute_reply":"2024-05-14T15:13:59.298115Z","shell.execute_reply.started":"2024-05-14T15:13:59.136760Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'ups': 115.0,\n"," 'num_comments': 23.0,\n"," 'upvote_ratio': 0.8799999952316284,\n"," 'score': 115.0,\n"," 'documents': 'this actually happened a couple of years ago. i grew up in germany where i went to a german secondary school that went from 5th to 13th grade (we still had 13 grades then, they have since changed that). my school was named after anne frank and we had a club that i was very active in from 9th grade on, which was dedicated to teaching incoming 5th graders about anne franks life, discrimination, anti-semitism, hitler, the third reich and that whole spiel. basically a day where the students\\' classes are cancelled and instead we give them an interactive history and social studies class with lots of activities and games. \\n\\nthis was my last year at school and i already had a lot of experience doing these project days with the kids. i was running the thing with a friend, so it was just the two of us and 30-something 5th graders. we start off with a brief introduction and brainstorming: what do they know about anne frank and the third reich? you\\'d be surprised how much they know. anyway after the brainstorming we do a few activities, and then we take a short break. after the break we split the class into two groups to make it easier to handle. one group watches a short movie about anne frank while the other gets a tour through our poster presentation that our student group has been perfecting over the years. then the groups switch. \\n\\ni\\'m in the classroom to show my group the movie and i take attendance to make sure no one decided to run away during break. i\\'m going down the list when i come to the name sandra (name changed). a kid with a boyish haircut and a somewhat deeper voice, wearing clothes from the boy\\'s section at a big clothing chain in germany, pipes up. \\n\\nnow keep in mind, these are all 11 year olds, they are all pre-pubescent, their bodies are not yet showing any sex specific features one would be able to see while they are fully clothed (e.g. boobs, beards,...). this being a 5th grade in the rather conservative (for german standards) bavaria, i was confused. i looked down at the list again making sure i had read the name right. look back up at the kid. \\n\\nme: \"you\\'re sandra?\"\\n\\nkid: \"yep.\"\\n\\nme: \"oh, sorry. *thinking the kid must be from somewhere where sandra is both a girl\\'s and boy\\'s name* where are you from? i\\'ve only ever heard that as a girl\\'s name before.\"\\n\\nthe class starts laughing. sandra gets really quiet. \"i am a girl...\" she says. some of the other students start saying that their parents made the same mistake when they met sandra. i feel so sorry and stupid. i get the class to calm down and finish taking attendance. we watch the movie in silence. after the movie, when we walked down to where the poster presentation took place i apologised to sandra. i felt so incredibly terrible, i still do to this day. throughout the rest of the day i heard lots of whispers about sandra. i tried to stop them whenever they came up, but there was no stopping the 5th grade gossip i had set in motion.\\n\\nsandra, if you\\'re out there, i am so incredibly sorry for humiliating you in front of your class. i hope you are happy and healthy and continue to live your life the way you like. don\\'t let anyone tell you you have to dress or act a certain way just because of the body parts you were born with. i\\'m sorry if i made you feel like you were wrong for dressing and acting differently. i\\'m sorry i probably made that day hell for you. i\\'m sorry for my ignorance.',\n"," 'tldr': 'confuse a 5th grade girl for a boy in front of half of her class. kids are mean. sorry sandra.**',\n"," 'title': 'gender-stereotyping'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["reddit_ds[0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:59.300972Z","iopub.status.busy":"2024-05-14T15:13:59.300498Z","iopub.status.idle":"2024-05-14T15:13:59.308642Z","shell.execute_reply":"2024-05-14T15:13:59.307784Z","shell.execute_reply.started":"2024-05-14T15:13:59.300939Z"},"trusted":true},"outputs":[],"source":["class RedditDataset(Dataset):\n","    def __init__(self, dataset, tokenizer, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.tokenizer.pad_token = tokenizer.eos_token\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        document = self.dataset[idx][\"documents\"]\n","        tokens = self.tokenizer(document, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        return tokens.input_ids.squeeze(), tokens.attention_mask.squeeze()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:13:59.310293Z","iopub.status.busy":"2024-05-14T15:13:59.309858Z","iopub.status.idle":"2024-05-14T15:14:01.636343Z","shell.execute_reply":"2024-05-14T15:14:01.635519Z","shell.execute_reply.started":"2024-05-14T15:13:59.310264Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c07de47e165e40d48470600c04e0723c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c10bdbc6f7ca42c68fc21f60f21c5849","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1320d11d4494e489beceb7c6a851c1f","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c00ffecc5e543d4bead3f5c19a8e1d9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e197da5bfc134548a2747a914ab36500","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(GPT2_PRESET)\n","train_ds = RedditDataset(reddit_ds, tokenizer, MAX_SEQUENCE_LENGTH)\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:01.637746Z","iopub.status.busy":"2024-05-14T15:14:01.637439Z","iopub.status.idle":"2024-05-14T15:14:01.647276Z","shell.execute_reply":"2024-05-14T15:14:01.646320Z","shell.execute_reply.started":"2024-05-14T15:14:01.637720Z"},"trusted":true},"outputs":[],"source":["# Callback for tracking GPU memory usage\n","class GPUMemoryCallback:\n","    def __init__(self, target_batches, print_stats=False):\n","        self.target_batches = target_batches\n","        self.print_stats = print_stats\n","        self.memory_usage = []\n","        self.labels = []\n","\n","    def compute_memory_usage(self):\n","        memory_stats = torch.cuda.memory_stats()\n","        peak_usage = round(memory_stats[\"allocated_bytes.all.peak\"] / (2**30), 3)\n","        self.memory_usage.append(peak_usage)\n","\n","    def on_epoch_begin(self, epoch):\n","        self.compute_memory_usage()\n","        self.labels.append(f\"epoch {epoch} start\")\n","\n","    def on_train_batch_begin(self, batch):\n","        if batch in self.target_batches:\n","            self.compute_memory_usage()\n","            self.labels.append(f\"batch {batch}\")\n","\n","    def on_epoch_end(self, epoch):\n","        self.compute_memory_usage()\n","        self.labels.append(f\"epoch {epoch} end\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:01.650939Z","iopub.status.busy":"2024-05-14T15:14:01.650633Z","iopub.status.idle":"2024-05-14T15:14:01.660744Z","shell.execute_reply":"2024-05-14T15:14:01.660015Z","shell.execute_reply.started":"2024-05-14T15:14:01.650915Z"},"trusted":true},"outputs":[],"source":["# Generate text\n","def generate_text(model, input_text, max_length=200):\n","    model.eval()\n","    start = time.time()\n","    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","    output = model.generate(inputs[\"input_ids\"], max_length=max_length)\n","    print(\"\\nOutput:\")\n","    print(tokenizer.decode(output[0], skip_special_tokens=True))\n","    end = time.time()\n","    print(f\"Total Time Elapsed: {end - start:.2f}s\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:01.661942Z","iopub.status.busy":"2024-05-14T15:14:01.661699Z","iopub.status.idle":"2024-05-14T15:14:01.671880Z","shell.execute_reply":"2024-05-14T15:14:01.670975Z","shell.execute_reply.started":"2024-05-14T15:14:01.661920Z"},"trusted":true},"outputs":[],"source":["# LoRA Layer\n","class LoraLayer(nn.Module):\n","    def __init__(self, original_layer, rank=8, alpha=32, trainable=False):\n","        super().__init__()\n","        self.rank = rank\n","        self.alpha = alpha\n","        self.scale = alpha / rank\n","\n","        self.original_layer = original_layer\n","        self.original_layer.requires_grad_(False)\n","\n","        self.A = nn.Linear(original_layer.in_features, rank, bias=False)\n","        self.B = nn.Linear(rank, original_layer.out_features, bias=False)\n","        self.B.weight.data.fill_(0.0)\n","\n","    def forward(self, x):\n","        original_output = self.original_layer(x)\n","        if self.training:\n","            lora_output = self.B(self.A(x)) * self.scale\n","            return original_output + lora_output\n","        return original_output\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:01.673526Z","iopub.status.busy":"2024-05-14T15:14:01.673011Z","iopub.status.idle":"2024-05-14T15:14:01.684684Z","shell.execute_reply":"2024-05-14T15:14:01.683811Z","shell.execute_reply.started":"2024-05-14T15:14:01.673495Z"},"trusted":true},"outputs":[],"source":["# Inject LoRA into model\n","def inject_lora(model, rank, alpha):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear) and 'attn' in name:\n","            setattr(model, name, LoraLayer(module, rank, alpha, trainable=True))\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:01.686821Z","iopub.status.busy":"2024-05-14T15:14:01.685803Z","iopub.status.idle":"2024-05-14T15:14:04.684078Z","shell.execute_reply":"2024-05-14T15:14:04.683213Z","shell.execute_reply.started":"2024-05-14T15:14:01.686794Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1496368ece14daf84f9b27b38caa9bf","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ddbda2e17d0b461b9b33dd4f445fbb68","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = GPT2LMHeadModel.from_pretrained(GPT2_PRESET).to(device)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:04.685395Z","iopub.status.busy":"2024-05-14T15:14:04.685145Z","iopub.status.idle":"2024-05-14T15:14:04.690384Z","shell.execute_reply":"2024-05-14T15:14:04.689503Z","shell.execute_reply.started":"2024-05-14T15:14:04.685373Z"},"trusted":true},"outputs":[],"source":["inject_lora(model, RANK, ALPHA)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:04.692038Z","iopub.status.busy":"2024-05-14T15:14:04.691685Z","iopub.status.idle":"2024-05-14T15:14:04.709244Z","shell.execute_reply":"2024-05-14T15:14:04.708359Z","shell.execute_reply.started":"2024-05-14T15:14:04.692006Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Optimizer and loss\n","optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * EPOCHS)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:04.710516Z","iopub.status.busy":"2024-05-14T15:14:04.710261Z","iopub.status.idle":"2024-05-14T15:14:04.719383Z","shell.execute_reply":"2024-05-14T15:14:04.718515Z","shell.execute_reply.started":"2024-05-14T15:14:04.710494Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Training\n","gpu_memory_callback = GPUMemoryCallback(target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500], print_stats=True)\n","model.train()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:14:04.721067Z","iopub.status.busy":"2024-05-14T15:14:04.720820Z","iopub.status.idle":"2024-05-14T15:58:49.707523Z","shell.execute_reply":"2024-05-14T15:58:49.706512Z","shell.execute_reply.started":"2024-05-14T15:14:04.721045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Batch 0, Loss: 4.09829044342041\n","Epoch 0, Batch 50, Loss: 3.407792091369629\n","Epoch 0, Batch 100, Loss: 3.5183229446411133\n","Epoch 0, Batch 150, Loss: 3.3907246589660645\n","Epoch 0, Batch 200, Loss: 3.2361392974853516\n","Epoch 0, Batch 250, Loss: 3.301361083984375\n","Epoch 0, Batch 300, Loss: 3.2553608417510986\n","Epoch 0, Batch 350, Loss: 3.2170135974884033\n","Epoch 0, Batch 400, Loss: 3.3598806858062744\n","Epoch 0, Batch 450, Loss: 3.3356807231903076\n","Epoch 0, Batch 500, Loss: 3.302633285522461\n","Epoch 0, Batch 550, Loss: 3.331789970397949\n","Epoch 0, Batch 600, Loss: 3.1675851345062256\n","Epoch 0, Batch 650, Loss: 3.2554142475128174\n","Epoch 0, Batch 700, Loss: 3.2382378578186035\n","Epoch 0, Batch 750, Loss: 3.37017822265625\n","Epoch 0, Batch 800, Loss: 3.305551767349243\n","Epoch 0, Batch 850, Loss: 3.2416396141052246\n","Epoch 0, Batch 900, Loss: 3.3602592945098877\n","Epoch 0, Batch 950, Loss: 3.288627862930298\n","Epoch 0, Batch 1000, Loss: 3.291459083557129\n","Epoch 0, Batch 1050, Loss: 3.272454023361206\n","Epoch 0, Batch 1100, Loss: 3.2419230937957764\n","Epoch 0, Batch 1150, Loss: 3.3243184089660645\n","Epoch 0, Batch 1200, Loss: 3.2836058139801025\n","Epoch 0, Batch 1250, Loss: 3.1856977939605713\n","Epoch 0, Batch 1300, Loss: 3.271883726119995\n","Epoch 1, Batch 0, Loss: 3.2259321212768555\n","Epoch 1, Batch 50, Loss: 3.1092002391815186\n","Epoch 1, Batch 100, Loss: 3.237973928451538\n","Epoch 1, Batch 150, Loss: 3.1229076385498047\n","Epoch 1, Batch 200, Loss: 3.189972400665283\n","Epoch 1, Batch 250, Loss: 3.1513161659240723\n","Epoch 1, Batch 300, Loss: 3.194396495819092\n","Epoch 1, Batch 350, Loss: 3.270585536956787\n","Epoch 1, Batch 400, Loss: 3.2300801277160645\n","Epoch 1, Batch 450, Loss: 3.228316068649292\n","Epoch 1, Batch 500, Loss: 3.1902806758880615\n","Epoch 1, Batch 550, Loss: 3.159013032913208\n","Epoch 1, Batch 600, Loss: 3.239187240600586\n","Epoch 1, Batch 650, Loss: 3.192305088043213\n","Epoch 1, Batch 700, Loss: 3.1757259368896484\n","Epoch 1, Batch 750, Loss: 3.192739963531494\n","Epoch 1, Batch 800, Loss: 3.2181613445281982\n","Epoch 1, Batch 850, Loss: 3.085205078125\n","Epoch 1, Batch 900, Loss: 3.2221336364746094\n","Epoch 1, Batch 950, Loss: 3.2512528896331787\n","Epoch 1, Batch 1000, Loss: 3.1053454875946045\n","Epoch 1, Batch 1050, Loss: 3.3338069915771484\n","Epoch 1, Batch 1100, Loss: 3.2198846340179443\n","Epoch 1, Batch 1150, Loss: 3.283463954925537\n","Epoch 1, Batch 1200, Loss: 3.196352481842041\n","Epoch 1, Batch 1250, Loss: 3.190598249435425\n","Epoch 1, Batch 1300, Loss: 3.0514469146728516\n","Epoch 2, Batch 0, Loss: 3.0750908851623535\n","Epoch 2, Batch 50, Loss: 3.1967933177948\n","Epoch 2, Batch 100, Loss: 3.030325174331665\n","Epoch 2, Batch 150, Loss: 2.928842782974243\n","Epoch 2, Batch 200, Loss: 3.13472318649292\n","Epoch 2, Batch 250, Loss: 3.151684284210205\n","Epoch 2, Batch 300, Loss: 3.136592388153076\n","Epoch 2, Batch 350, Loss: 3.169517993927002\n","Epoch 2, Batch 400, Loss: 3.175283908843994\n","Epoch 2, Batch 450, Loss: 3.2396016120910645\n","Epoch 2, Batch 500, Loss: 3.126619577407837\n","Epoch 2, Batch 550, Loss: 3.12131667137146\n","Epoch 2, Batch 600, Loss: 3.142601728439331\n","Epoch 2, Batch 650, Loss: 3.046043872833252\n","Epoch 2, Batch 700, Loss: 3.227663278579712\n","Epoch 2, Batch 750, Loss: 3.0726699829101562\n","Epoch 2, Batch 800, Loss: 3.164153814315796\n","Epoch 2, Batch 850, Loss: 3.2150685787200928\n","Epoch 2, Batch 900, Loss: 3.1421589851379395\n","Epoch 2, Batch 950, Loss: 3.162327289581299\n","Epoch 2, Batch 1000, Loss: 3.204308032989502\n","Epoch 2, Batch 1050, Loss: 3.103201150894165\n","Epoch 2, Batch 1100, Loss: 3.204568386077881\n","Epoch 2, Batch 1150, Loss: 3.1936264038085938\n","Epoch 2, Batch 1200, Loss: 3.091496706008911\n","Epoch 2, Batch 1250, Loss: 3.0802860260009766\n","Epoch 2, Batch 1300, Loss: 3.1172919273376465\n"]}],"source":["for epoch in range(EPOCHS):\n","    gpu_memory_callback.on_epoch_begin(epoch)\n","    for batch_idx, (input_ids, attention_mask) in enumerate(train_loader):\n","        gpu_memory_callback.on_train_batch_begin(batch_idx)\n","        optimizer.zero_grad()\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        if batch_idx % 50 == 0:\n","            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n","    gpu_memory_callback.on_epoch_end(epoch)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:58:49.709586Z","iopub.status.busy":"2024-05-14T15:58:49.709160Z","iopub.status.idle":"2024-05-14T15:58:49.713830Z","shell.execute_reply":"2024-05-14T15:58:49.712865Z","shell.execute_reply.started":"2024-05-14T15:58:49.709510Z"},"trusted":true},"outputs":[],"source":["lora_model_memory_usage = gpu_memory_callback.memory_usage\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:08:30.649646Z","iopub.status.busy":"2024-05-14T16:08:30.648948Z","iopub.status.idle":"2024-05-14T16:08:30.655628Z","shell.execute_reply":"2024-05-14T16:08:30.654611Z","shell.execute_reply.started":"2024-05-14T16:08:30.649612Z"},"trusted":true},"outputs":[],"source":["# Merge LoRA weights\n","def merge_lora_weights(model):\n","    for name, module in model.named_modules():\n","        if isinstance(module, LoraLayer):\n","            module.original_layer.weight.data += torch.mm(module.A.weight.data.T, module.B.weight.data) * module.scale\n","\n","merge_lora_weights(model)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:27:23.629229Z","iopub.status.busy":"2024-05-14T16:27:23.628348Z","iopub.status.idle":"2024-05-14T16:27:25.560514Z","shell.execute_reply":"2024-05-14T16:27:25.559443Z","shell.execute_reply.started":"2024-05-14T16:27:23.629191Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output:\n","I woke up in the morning and was feeling a little bit tired. i was feeling a little bit tired, so i decided to go to the bathroom. i was feeling a little bit tired, so\n","Total Time Elapsed: 0.36s\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output:\n","Me and my girlfriend went to a local bar to have a few drinks. we were drinking a lot and i was feeling a little drunk. i was drinking a lot and i was feeling a little drunk\n","Total Time Elapsed: 0.37s\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output:\n","I really enjoy this subreddit, and i'm not sure if i should post it here or not. \n","\n","so i'm a college student, and i'm a student in a small university. i'm a freshman in high school, and i\n","Total Time Elapsed: 0.53s\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output:\n","My memorable moment of my life happened about a year ago. i was in my first year of college and i was in the middle of a class project. i was in the middle of a class project\n","Total Time Elapsed: 0.38s\n","\n","Output:\n","The most crazy story i've ever told is that i was in a relationship with a girl i had been seeing for a few months. i was in\n","Total Time Elapsed: 0.28s\n"]}],"source":["generate_text(model, \"I woke up in the morning and\", max_length=40)\n","generate_text(model, \"Me and my girlfriend went to\", max_length=40)\n","generate_text(model, \"I really enjoy\", max_length=50)\n","generate_text(model, \"My memorable moment\", max_length=40)\n","generate_text(model, \"The most crazy story\", max_length=30)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
